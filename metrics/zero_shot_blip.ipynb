{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "import math\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torchvision\n",
    "import clip\n",
    "from clip_lt.utils.labels_names import labels_names\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import time\n",
    "from clip_lt.utils.labels_names import labels_names\n",
    "import glob\n",
    "from clip_lt.blip.models.blip_itm import blip_itm\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_size = 384\n",
    "\n",
    "\n",
    "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n",
    "\n",
    "model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n",
    "model.eval()\n",
    "model = model.to(device=device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset_dir_path = '/Volumes/black_ssd/datasets/imagenet_lt/'\n",
    "dataset_dir_path = '/Users/rotemisraeli/Documents/datasets/imagenet_lt/'\n",
    "\n",
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225] )\n",
    "])\n",
    "\n",
    "text_features = torch.load('../text_features2.pt')\n",
    "text_features = (text_features / text_features.norm(dim=-1, keepdim=True)).t().to(device)\n",
    "logit_scale = (nn.Parameter(torch.ones([]) * np.log(1 / 0.07))).exp().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "good = 0.0\n",
    "bad = 0.0\n",
    "with torch.no_grad():\n",
    "    tqdm_ = tqdm(range(1000))\n",
    "    for label in tqdm_:\n",
    "        for image_name in glob.glob(f'{dataset_dir_path}val/{label}/*'):\n",
    "            image = preprocess(Image.open(image_name)).unsqueeze(0).to(device)\n",
    "            torchvision.utils.save_image(image,'test2.png')\n",
    "            image_features = clip_model.encode_image(image)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            logits_per_image = logit_scale * image_features @ text_features\n",
    "            probs = logits_per_image.softmax(dim=-1)\n",
    "            predicted = torch.argmax(probs)\n",
    "            if predicted == label:\n",
    "                good += 1.0\n",
    "            else:\n",
    "                bad += 1.0\n",
    "            tqdm_.desc = f'{100*good/(good+bad)}'\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "out = model(torch.zeros(1,3,image_size,image_size),'test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}