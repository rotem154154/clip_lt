{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "CLIP(\n  (visual): VisionTransformer(\n    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (transformer): Transformer(\n      (resblocks): Sequential(\n        (0): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (transformer): Transformer(\n    (resblocks): Sequential(\n      (0): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (6): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (7): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (8): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (9): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (10): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (11): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (token_embedding): Embedding(49408, 512)\n  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "import math\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torchvision\n",
    "import clip\n",
    "from clip_lt.utils.labels_names import labels_names\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load('ViT-B/32', device)\n",
    "clip_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# dataset_dir_path = '/Volumes/black_ssd/datasets/imagenet_lt/'\n",
    "dataset_dir_path = '/Users/rotemisraeli/Documents/datasets/TinyImageNet/'\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(dataset_dir_path+'val/',transform=preprocess)\n",
    "val_data_loader = DataLoader(test_data, batch_size=1,num_workers=4)\n",
    "train_data = torchvision.datasets.ImageFolder(dataset_dir_path+'train/',transform=preprocess)\n",
    "train_data_loader = DataLoader(train_data, batch_size=1,num_workers=4)\n",
    "\n",
    "text_features = torch.load('../text_features.pt')\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "logit_scale = (nn.Parameter(torch.ones([]) * np.log(1 / 0.07))).exp()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100,shuffle=True)):\n",
    "            features = clip_model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [02:01<22:49, 14.88s/it]"
     ]
    }
   ],
   "source": [
    "test_features, test_labels = get_features(test_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_features, train_labels = get_features(train_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_features)\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}