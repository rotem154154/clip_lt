{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rotemisraeli/opt/anaconda3/envs/env1/lib/python3.8/site-packages/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended\n",
      "  warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torchvision\n",
    "import clip\n",
    "from clip_lt.utils.labels_names import labels_names\n",
    "import torch.nn as nn\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset_dir_path = '/Volumes/black_ssd/datasets/imagenet_lt/'\n",
    "dataset_dir_path = '/Users/rotemisraeli/Documents/datasets/imagenet_lt/'\n",
    "\n",
    "class LightningMNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, config, data_dir=None):\n",
    "        super(LightningMNISTClassifier, self).__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.lr = config['lr']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        self.text_features = torch.load('../text_features.pt')\n",
    "        self.text_features = self.text_features / self.text_features.norm(dim=-1, keepdim=True)\n",
    "        self.logit_scale = (nn.Parameter(torch.ones([]) * np.log(1 / 0.07))).exp()\n",
    "        self.linear_layer = nn.Linear(512,1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(x)\n",
    "        out = self.linear_layer(image_features)\n",
    "        out = out.softmax(dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def old_forward(self, x):\n",
    "        image_features = self.clip_model.encode_image(x)\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logits_per_image = self.logit_scale * image_features @ self.text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        probs = logits_per_image#.softmax(dim=-1)\n",
    "        # print(probs.shape,probs)\n",
    "        return probs\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "    def accuracy(self, logits, labels):\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / len(labels)\n",
    "        return torch.tensor(accuracy)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        self.log(\"ptl/train_accuracy\", accuracy,prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # TRANSFORM_IMG = transforms.Compose([\n",
    "        #     transforms.Resize(224),\n",
    "        #     transforms.CenterCrop(224),\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                          std=[0.229, 0.224, 0.225] )\n",
    "        # ])\n",
    "        train_data = torchvision.datasets.ImageFolder(self.data_dir+'train/',transform=self.clip_preprocess)\n",
    "        return DataLoader(train_data, batch_size=int(self.batch_size),num_workers=1,shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        train_data = torchvision.datasets.ImageFolder(self.data_dir+'val/',transform=self.clip_preprocess)\n",
    "        return DataLoader(train_data, batch_size=int(self.batch_size),num_workers=1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def train_mnist(config):\n",
    "    model = LightningMNISTClassifier(config,data_dir=dataset_dir_path)\n",
    "    trainer = pl.Trainer(max_epochs=1, logger=wandb_logger)\n",
    "\n",
    "    trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train_mnist_no_tune():\n",
    "    config = {\n",
    "        \"layer_1_size\": 128,\n",
    "        \"layer_2_size\": 256,\n",
    "        \"lr\": 4e-3,\n",
    "        \"batch_size\": 32\n",
    "    }\n",
    "    train_mnist(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mrotem98\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/rotemisraeli/Documents/local_python/clip_lt/trainer/wandb/run-20220405_152358-38jmsnqf</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/rotem98/clip_lt-trainer/runs/38jmsnqf\" target=\"_blank\">avid-dawn-18</a></strong> to <a href=\"https://wandb.ai/rotem98/clip_lt-trainer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type   | Params\n",
      "----------------------------------------\n",
      "0 | clip_model   | CLIP   | 151 M \n",
      "1 | linear_layer | Linear | 513 K \n",
      "----------------------------------------\n",
      "151 M     Trainable params\n",
      "0         Non-trainable params\n",
      "151 M     Total params\n",
      "607.161   Total estimated model params size (MB)\n",
      "/Users/rotemisraeli/opt/anaconda3/envs/env1/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation sanity check: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08dc49eacd4744fd8764958bd029991a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rotemisraeli/opt/anaconda3/envs/env1/lib/python3.8/site-packages/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended\n",
      "  warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n",
      "/Users/rotemisraeli/opt/anaconda3/envs/env1/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f9d72e785da4d9ca4ef51ddcc52a458"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mnist_no_tune()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "texts = []\n",
    "for i in range(1000):\n",
    "    label_name = labels_names[i].split(',')[0]\n",
    "    if label_name[0] in 'aouie':\n",
    "        texts.append(f'a photo of an {label_name}')\n",
    "    else:\n",
    "        texts.append(f'a photo of a {label_name}')\n",
    "\n",
    "# texts2 = clip.tokenize(texts).to(device)\n",
    "# with torch.no_grad():\n",
    "#     text_features = model.encode_text(texts2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(text_features,'text_features2.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "texts[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clip_model.visual.features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}